Throttle explanation (a little involved & probably more info than you really wanted to know ... ):

(1) When doing data streaming, we use buffers to hold the data between blocks. These buffers are (of course) finite in size; so before any block can do "work", it must check the amount of available input data and output buffer space. If there is not enough input data, or not enough output buffer space then no "work" takes place. We call this situation "backpressure".

(2) Flowgraphs can consist of combinations of streaming and messages. It is possible to use multiple of both in a single flowgraph, though one has to be careful in doing so! Both will, in general, process data as fast as possible without regard to desired or necessary sample rate; we generally just hope that the processing data rate is faster than the required sample rate & we call this "real time signal processing" if it is the case (and "offline" processing otherwise, since the processing can't keep up with the required data rate & thus would normally generate underruns).

(3) Certain sinks and sources provide data at a given sample rate; these include audio and UHD/USRP. We call such blocks "clocks", because they reliably provide or consume data at some given (and generally configurable) rate. In a single streaming flowgraph portion, most flowgraphs will have just a single such source or sink, because otherwise we have multiple clocks whose sample rates must be simultaneously met; this is called the "2 clock problem", and there is no good solution as of yet (there -are- solutions, but they are complicated & CPU intensive & not in the spirit of GR).

(4) If your flowgraph has no external clock via (3), then it generally should have a throttle to act as a clock. Without such flow control, the data in the flowgraph will move as quickly as the CPU can processing it, which if the flowgraph is complicated enough can result in the system seeming to hang / become non-responsive, since all available CPU time will be spent processing the flowgraph.

(5) When your flowgraph has a single clock -- regartdless of whether (3) or (4) -- and assuming processing is happening in (at least) real time, then you can visualize the data flowing through the buffers as a relatively continuous 2-dimensional plot: pipeline (moving throught the flowgraph) & waterfall (time), the "fullness" of which depends on how close to "real time" the processing is happening: if processing is -much- faster than the data rate, then the visualization is more of a pulse as the data moves through the flowgraph; as the data rate approaches the processing capabilities, the pulses become more of a continuous pipeline and thus more of a waterfall too. When the data rate is very close to the processing capabilities, the visualization is almost entirely filled up with data flowing across all blocks at all times, with just small gaps here and there.

(6) When doing data streaming for simulation purposes -- meaning with no clocks from (3) -- then instead of simulating the source/sink with a non-clocked input/output combined with a throttle, we generally move the throttle to a single spot where it can act for both input and output. The flowgraph up until the throttle will initially execute as quickly as the CPU and do processing, but eventually the backpressure will result in the average throughput being whatever the throttle is set at, via the pipeline in (5) being as full as possible with data & thus processing (1) happening only when possible. The flowgraph after the throttle will run like a normal clocked flowgraph as in (5).

(7) Multiple throttles are like multiple clocks from (3): they will compete for data and, since the clocks cannot be -exactly- the same data rate, the flowgraph will eventually either underrun or overflow, depending on which throttle is flowing faster.

(8) Thus, although it seems natural to create a virtual sink/source by using a throttle, it's generally not a good idea. Inserting a async message flowgraph so split the streaming into 2 parts may or not help with (7); really depends on what you're trying to do.


======================



(9) "Data streaming" makes one think of a very long stream of data being processed; so long that it for all practicaly purposes has no end. Most GNU Radio flowgraphs are designed with this concept in mind ... but, what if you just want to transfer a file, nothing more or less.

(10) For most packet-based communication systems, we take a payload and join it with a header and then maybe modulate the whole shebang. For small payloads, the ratio of header data to payload data is "high", while for large payloads, the ratio is "low". This means that a bigger payload is more efficient in terms of overhead used for a given amount of data transferred. The tradeoff for real-life communications is that the probability of a packet decoding error increases with a bigger payload.

(11) Hence, while we'd love to transfer a whole file as a single packet, it's really not realistic to do so, because the probability of the whole file successfully being decoded decreases as the payload length increases.

(12) Thus, we split the file into chunks of data, typically 4-7 times the header length in bytes, as a trade-off between wanting max payload length yet minimal probability of a packet decoding error. Many communications system have a set header:payload length ratio; some are variable.

(13) Inside GNU Radio, transferring a file results in the file being split into a finite number of packets via "stream to tagged stream" for length N input bytes. Inside this block, it will clump N input bytes into a tagged group. For an "infinite" stream, this isn't an issue ... N byte chunks keep getting clumped! But for a file of length not equal to M*N for M an integer, this block will output M-1 tagged stream chunks, then sit waiting for the residual data bytes to get to N ... that final M'th tagged stream chunk won't ever be output! Of course if the file length is exactly divisible by N, then this is OK.

(14) There is an "end of stream" setting that is used in GNU Radio, which indicates for those blocks that respond to it to finalize the block's processing given its current state & then propagate the command downstream in the flowgraph. Not all sources will generated this state, and not all blocks can use it. I'm pretty sure the "stream to tagged stream" block doesn't handle the state.

(15) For each tagged stream chunk on the Tx side: Because we're using tagged streams all the way from file split to full encoded packet output, all of the file data excepting possibly the final M'th tagged stream chunk will be exactly packetized and encoded.


======================


Here’s my understanding:
(2): The Burst Shaper is meant to change the bandwidth used for each output bit, as well as padding bursts to account for physically-determined characteristics such as multipath duration. Especially for your simulation, the Burst Shaper doesn’t really do very much, since you have effectively infinite BW; it is used for practical communication schemes, for example using a USRP Tx -> Rx and going fully wireless!
So if for your thesis you can just do simulations of channels, you don’t really need the Burst Shaper
(1) Using the “Polyphase Clock Sync” and the “Costas Loop”  should work nicely. The former takes care of timing sync while the latter handles frequency sync. These are especially good for the simulated channel in your example right now, but I think they will also work nicely using a USRP Tx -> Rx and going fully wireless. Certainly worth trying.
The HPD does create an unnecessary cycle that just slows things down. Further, when there are close to continuous packets coming in, the HPD can totally lose it! Best to use something like what you’ve moved to.